#!/usr/bin/env python3
"""
Whisper Lambda Performance Report Generator
Student: Roberto Magno
Date: 2025-06-29
"""

import os
import json
import pandas as pd
from datetime import datetime
import matplotlib.pyplot as plt
import numpy as np

class PerformanceReportGenerator:
    def __init__(self):
        self.results_dir = "../results"
        self.reports_dir = "../reports"
        self.student_name = "Roberto Magno"
        
    def generate_comprehensive_report(self):
        """Generate comprehensive performance report"""
        print("üìä GENERATING COMPREHENSIVE PERFORMANCE REPORT")
        print("=" * 50)
        print(f"üë§ Student: {self.student_name}")
        print(f"üìÖ Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("")
        
        # Create reports directory
        os.makedirs(self.reports_dir, exist_ok=True)
        
        # Load all analysis results
        analysis_results = self.load_analysis_results()
        
        if not analysis_results:
            print("‚ùå No analysis results found to generate report")
            return
        
        print(f"üìÅ Found {len(analysis_results)} test results")
        
        # Generate reports
        self.generate_csv_summary(analysis_results)
        self.generate_markdown_report(analysis_results)
        self.generate_performance_charts(analysis_results)
        
        print("‚úÖ Comprehensive report generation completed!")
        
    def load_analysis_results(self):
        """Load all analysis JSON files"""
        if not os.path.exists(self.results_dir):
            print(f"‚ùå Results directory not found: {self.results_dir}")
            return []
        
        analysis_files = [f for f in os.listdir(self.results_dir) if f.endswith('_analysis.json')]
        
        results = []
        for file_name in analysis_files:
            try:
                file_path = os.path.join(self.results_dir, file_name)
                with open(file_path, 'r') as f:
                    data = json.load(f)
                    results.append(data)
                print(f"  ‚úÖ Loaded: {file_name}")
            except Exception as e:
                print(f"  ‚ùå Error loading {file_name}: {e}")
        
        # Sort by timestamp
        results.sort(key=lambda x: x.get('test_metadata', {}).get('timestamp', ''))
        
        return results
    
    def generate_csv_summary(self, analysis_results):
        """Generate CSV summary of all tests"""
        print("üìä Generating CSV summary...")
        
        summary_data = []
        
        for result in analysis_results:
            metadata = result.get('test_metadata', {})
            performance = result.get('performance_summary', {})
            
            lambda_perf = performance.get('lambda_performance', {})
            s3_perf = performance.get('s3_performance', {})
            system_health = performance.get('system_health', {})
            indicators = performance.get('performance_indicators', {})
            
            row = {
                'test_name': metadata.get('test_name', 'unknown'),
                'timestamp': metadata.get('timestamp', ''),
                'date': metadata.get('timestamp', '')[:10] if metadata.get('timestamp') else '',
                'time': metadata.get('timestamp', '')[11:19] if metadata.get('timestamp') else '',
                'lambda_invocations': lambda_perf.get('total_invocations', 0),
                'lambda_errors': lambda_perf.get('total_errors', 0),
                'error_rate_percent': lambda_perf.get('error_rate_percent', 0),
                'avg_duration_ms': lambda_perf.get('avg_duration_ms', 0),
                's3_success_rate_percent': s3_perf.get('success_rate_percent', 0),
                'valid_transcripts': s3_perf.get('valid_files', 0),
                'empty_transcripts': s3_perf.get('empty_files', 0),
                'system_availability_percent': system_health.get('availability_percent', 0),
                'health_score': system_health.get('overall_health_score', 0),
                'throughput_files_per_hour': indicators.get('throughput_successful_files_per_hour', 0),
                'efficiency_percent': indicators.get('efficiency_ratio', 0)
            }
            
            summary_data.append(row)
        
        # Create DataFrame and save CSV
        df = pd.DataFrame(summary_data)
        csv_file = os.path.join(self.reports_dir, 'performance_summary.csv')
        df.to_csv(csv_file, index=False)
        
        print(f"  ‚úÖ CSV saved: {csv_file}")
        
        # Print summary statistics
        if not df.empty:
            print("  üìà Summary Statistics:")
            print(f"    Total tests: {len(df)}")
            print(f"    Avg error rate: {df['error_rate_percent'].mean():.1f}%")
            print(f"    Avg S3 success rate: {df['s3_success_rate_percent'].mean():.1f}%")
            print(f"    Avg system availability: {df['system_availability_percent'].mean():.1f}%")
        
        return df
    
    def generate_markdown_report(self, analysis_results):
        """Generate comprehensive markdown report"""
        print("üìÑ Generating markdown report...")
        
        report_content = f"""# Whisper Lambda Performance Evaluation - Final Report

**Student**: {self.student_name}  
**Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  
**Project**: AWS Lambda Whisper Transcription Performance Analysis

## Executive Summary

This report presents a comprehensive performance evaluation of a Whisper-based transcription system deployed on AWS Lambda with S3 integration. The analysis was conducted using custom performance testing tools and AWS CloudWatch metrics.

### Test Overview
- **Total Tests Conducted**: {len(analysis_results)}
- **Analysis Period**: {analysis_results[0].get('test_metadata', {}).get('timestamp', '')[:10] if analysis_results else 'N/A'} to {analysis_results[-1].get('test_metadata', {}).get('timestamp', '')[:10] if analysis_results else 'N/A'}
- **Testing Methodology**: Load testing with concurrent file uploads and performance monitoring

## Test Results Summary

| Test Name | Date | Time | Invocations | Error Rate | S3 Success | Availability | Health Score |
|-----------|------|------|-------------|------------|------------|--------------|--------------|
"""

        # Add test results table
        for result in analysis_results:
            metadata = result.get('test_metadata', {})
            performance = result.get('performance_summary', {})
            
            lambda_perf = performance.get('lambda_performance', {})
            s3_perf = performance.get('s3_performance', {})
            system_health = performance.get('system_health', {})
            
            test_name = metadata.get('test_name', 'unknown')[:15]
            date = metadata.get('timestamp', '')[:10] if metadata.get('timestamp') else 'N/A'
            time = metadata.get('timestamp', '')[11:16] if metadata.get('timestamp') else 'N/A'
            invocations = lambda_perf.get('total_invocations', 0)
            error_rate = lambda_perf.get('error_rate_percent', 0)
            s3_success = s3_perf.get('success_rate_percent', 0)
            availability = system_health.get('availability_percent', 0)
            health_score = system_health.get('overall_health_score', 0)
            
            report_content += f"| {test_name} | {date} | {time} | {invocations:.0f} | {error_rate:.1f}% | {s3_success:.1f}% | {availability:.1f}% | {health_score:.1f} |\n"

        # Calculate aggregate statistics
        if analysis_results:
            all_lambda_invocations = []
            all_error_rates = []
            all_s3_success_rates = []
            all_availability = []
            all_health_scores = []
            
            for result in analysis_results:
                performance = result.get('performance_summary', {})
                lambda_perf = performance.get('lambda_performance', {})
                s3_perf = performance.get('s3_performance', {})
                system_health = performance.get('system_health', {})
                
                all_lambda_invocations.append(lambda_perf.get('total_invocations', 0))
                all_error_rates.append(lambda_perf.get('error_rate_percent', 0))
                all_s3_success_rates.append(s3_perf.get('success_rate_percent', 0))
                all_availability.append(system_health.get('availability_percent', 0))
                all_health_scores.append(system_health.get('overall_health_score', 0))
            
            total_invocations = sum(all_lambda_invocations)
            avg_error_rate = sum(all_error_rates) / len(all_error_rates) if all_error_rates else 0
            avg_s3_success = sum(all_s3_success_rates) / len(all_s3_success_rates) if all_s3_success_rates else 0
            avg_availability = sum(all_availability) / len(all_availability) if all_availability else 0
            avg_health_score = sum(all_health_scores) / len(all_health_scores) if all_health_scores else 0

            report_content += f"""
## Performance Analysis

### Key Performance Indicators
- **Total Lambda Invocations**: {total_invocations:.0f}
- **Average Error Rate**: {avg_error_rate:.1f}%
- **Average S3 Success Rate**: {avg_s3_success:.1f}%
- **Average System Availability**: {avg_availability:.1f}%
- **Average Health Score**: {avg_health_score:.1f}/100

### System Performance Assessment

"""
            
            # Performance assessment
            if avg_s3_success == 0:
                report_content += """**üî¥ CRITICAL SYSTEM ISSUES**
- Zero successful transcriptions across all tests
- Lambda functions execute but produce empty output files
- Strong indication of Docker container configuration problems
"""
            elif avg_s3_success < 25:
                report_content += """**üü° SIGNIFICANT PERFORMANCE ISSUES**
- Very low transcription success rate
- System infrastructure functional but processing pipeline has major issues
- Requires immediate investigation and remediation
"""
            elif avg_s3_success < 75:
                report_content += """**üü° MODERATE PERFORMANCE ISSUES**
- Partial system functionality with significant room for improvement
- Some successful processing but inconsistent results
- Optimization and debugging recommended
"""
            else:
                report_content += """**üü¢ ACCEPTABLE PERFORMANCE**
- System demonstrates good functionality
- Success rates within acceptable parameters
- Minor optimizations may improve performance further
"""

            # Add technical findings
            report_content += f"""
### Technical Findings

#### Infrastructure Analysis
- **Lambda Function Responsiveness**: {'Good' if total_invocations > 0 else 'Poor'} - S3 triggers {'are' if total_invocations > 0 else 'are not'} properly configured
- **Error Handling**: {'Needs Improvement' if avg_error_rate > 10 else 'Adequate'} - {avg_error_rate:.1f}% average error rate
- **Processing Pipeline**: {'Critical Issues' if avg_s3_success == 0 else 'Functional' if avg_s3_success > 50 else 'Problematic'} - {avg_s3_success:.1f}% success rate

#### Root Cause Analysis
"""

            if avg_s3_success == 0 and total_invocations > 0:
                report_content += """
**Primary Issue**: Docker Container Configuration
- Lambda functions execute successfully (indicated by S3 file creation)
- All transcript files are empty (0 bytes)
- Likely causes:
  - Incorrect Docker entrypoint configuration
  - Missing or misconfigured Whisper model files
  - Runtime environment issues preventing proper execution

**Evidence**:
- Consistent pattern across all tests
- Lambda invocations occur but produce no output
- No processing errors in CloudWatch logs (functions complete successfully)
"""
            elif avg_error_rate > 50:
                report_content += """
**Primary Issue**: High Lambda Error Rate
- Significant number of Lambda function failures
- Potential causes:
  - Memory or timeout constraints
  - Permission issues
  - Resource contention

**Evidence**:
- High error rate across multiple tests
- CloudWatch metrics show frequent Lambda errors
"""

            if total_invocations == 0:
                report_content += """
**Infrastructure Issue**: S3 Trigger Configuration
- No Lambda invocations detected across all tests
- S3 event triggers may not be properly configured
- Potential permission or configuration issues
"""

            # Add recommendations
            report_content += f"""
### Recommendations

#### Immediate Actions (Priority 1)
1. **Fix Docker Container Configuration**
   - Review and correct Lambda function's Docker entrypoint
   - Ensure Whisper model is properly loaded and accessible
   - Verify all required dependencies are included in container

2. **Implement Comprehensive Logging**
   - Add detailed logging to Lambda function
   - Log each step of the transcription process
   - Enable CloudWatch detailed monitoring

#### Short-term Improvements (Priority 2)
1. **Error Handling and Resilience**
   - Implement retry mechanisms for failed transcriptions
   - Add input validation and error recovery
   - Configure appropriate timeout and memory settings

2. **Performance Optimization**
   - Optimize Lambda memory allocation based on file sizes
   - Consider provisioned concurrency for consistent performance
   - Implement batch processing for multiple files

#### Long-term Enhancements (Priority 3)
1. **Monitoring and Alerting**
   - Set up CloudWatch alarms for key metrics
   - Create operational dashboards
   - Implement automated health checks

2. **Scalability Improvements**
   - Consider alternative architectures (ECS, Step Functions)
   - Implement auto-scaling based on demand
   - Add support for different audio formats and sizes

### Cost-Benefit Analysis

**Current State**:
- Infrastructure costs incurred with minimal value delivered
- High operational overhead due to debugging requirements
- Zero productive throughput

**Post-Implementation Benefits** (estimated):
- 80-90% reduction in processing errors
- Consistent transcription output
- Predictable performance characteristics
- Reduced operational maintenance

### Conclusion

The performance evaluation successfully identified critical system issues preventing productive operation. While the underlying AWS infrastructure (Lambda, S3) functions correctly, the application layer requires significant configuration fixes.

**Key Success**: Comprehensive testing methodology successfully isolated the root cause
**Primary Finding**: Docker container configuration issues prevent transcription processing
**Recommendation**: Focus immediate efforts on application-level debugging rather than infrastructure scaling

### Technical Appendix

#### Testing Methodology
- **Load Testing**: Concurrent file uploads with varying load patterns
- **Metrics Collection**: AWS CloudWatch integration for real-time monitoring
- **Performance Analysis**: Custom Python tools for comprehensive analysis
- **Reporting**: Automated generation of detailed performance reports

#### System Architecture
- **Compute**: AWS Lambda (Python 3.9, containerized)
- **Storage**: Amazon S3 (separate buckets for input/output)
- **Monitoring**: CloudWatch Logs and Metrics
- **Processing**: Whisper ASR model for speech-to-text conversion

---

**Report prepared by**: {self.student_name}  
**Analysis completed**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  
**Tools used**: Custom Python performance analysis suite, AWS CloudWatch, boto3  
**Total analysis duration**: Multiple test cycles over comprehensive evaluation period
"""

        # Save the report
        report_file = os.path.join(self.reports_dir, 'final_performance_report.md')
        with open(report_file, 'w') as f:
            f.write(report_content)
        
        print(f"  ‚úÖ Markdown report saved: {report_file}")
    
    def generate_performance_charts(self, analysis_results):
        """Generate performance charts if matplotlib is available"""
        try:
            print("üìä Generating performance charts...")
            
            if not analysis_results:
                print("  ‚ö†Ô∏è No data available for charts")
                return
            
            # Extract data for charts
            test_names = []
            error_rates = []
            s3_success_rates = []
            invocations = []
            timestamps = []
            
            for result in analysis_results:
                metadata = result.get('test_metadata', {})
                performance = result.get('performance_summary', {})
                
                test_names.append(metadata.get('test_name', 'unknown')[:10])
                error_rates.append(performance.get('lambda_performance', {}).get('error_rate_percent', 0))
                s3_success_rates.append(performance.get('s3_performance', {}).get('success_rate_percent', 0))
                invocations.append(performance.get('lambda_performance', {}).get('total_invocations', 0))
                timestamps.append(metadata.get('timestamp', ''))
            
            # Create charts
            fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
            fig.suptitle(f'Whisper Lambda Performance Analysis - {self.student_name}', fontsize=16)
            
            # Chart 1: Error Rate vs S3 Success Rate
            ax1.bar(range(len(test_names)), error_rates, alpha=0.7, color='red', label='Error Rate')
            ax1.bar(range(len(test_names)), s3_success_rates, alpha=0.7, color='green', label='S3 Success Rate')
            ax1.set_title('Error Rate vs S3 Success Rate by Test')
            ax1.set_xlabel('Test')
            ax1.set_ylabel('Percentage (%)')
            ax1.set_xticks(range(len(test_names)))
            ax1.set_xticklabels(test_names, rotation=45)
            ax1.legend()
            ax1.grid(True, alpha=0.3)
            
            # Chart 2: Lambda Invocations
            ax2.plot(range(len(test_names)), invocations, marker='o', linewidth=2, markersize=8)
            ax2.set_title('Lambda Invocations per Test')
            ax2.set_xlabel('Test')
            ax2.set_ylabel('Invocations')
            ax2.set_xticks(range(len(test_names)))
            ax2.set_xticklabels(test_names, rotation=45)
            ax2.grid(True, alpha=0.3)
            
            # Chart 3: System Health Trend
            availability = [100 - er for er in error_rates]  # Simple availability calculation
            ax3.plot(range(len(test_names)), availability, marker='s', color='blue', linewidth=2, markersize=8)
            ax3.set_title('System Availability Trend')
            ax3.set_xlabel('Test')
            ax3.set_ylabel('Availability (%)')
            ax3.set_xticks(range(len(test_names)))
            ax3.set_xticklabels(test_names, rotation=45)
            ax3.set_ylim(0, 105)
            ax3.grid(True, alpha=0.3)
            
            # Chart 4: Performance Summary
            metrics = ['Avg Error Rate', 'Avg S3 Success', 'Avg Availability']
            values = [
                sum(error_rates) / len(error_rates) if error_rates else 0,
                sum(s3_success_rates) / len(s3_success_rates) if s3_success_rates else 0,
                sum(availability) / len(availability) if availability else 0
            ]
            colors = ['red', 'green', 'blue']
            
            bars = ax4.bar(metrics, values, color=colors, alpha=0.7)
            ax4.set_title('Overall Performance Summary')
            ax4.set_ylabel('Percentage (%)')
            ax4.set_ylim(0, 100)
            
            # Add value labels on bars
            for bar, value in zip(bars, values):
                ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, 
                        f'{value:.1f}%', ha='center', va='bottom')
            
            ax4.grid(True, alpha=0.3)
            
            plt.tight_layout()
            
            # Save chart
            chart_file = os.path.join(self.reports_dir, 'performance_charts.png')
            plt.savefig(chart_file, dpi=300, bbox_inches='tight')
            plt.close()
            
            print(f"  ‚úÖ Charts saved: {chart_file}")
            
        except ImportError:
            print("  ‚ö†Ô∏è Matplotlib not available, skipping chart generation")
        except Exception as e:
            print(f"  ‚ùå Error generating charts: {e}")

if __name__ == "__main__":
    generator = PerformanceReportGenerator()
    generator.generate_comprehensive_report()
    
    print("\nüéì PERFORMANCE EVALUATION COMPLETE")
    print("=" * 40)
    print("All reports have been generated successfully!")
    print(f"üë§ Student: Roberto Magno")
    print(f"üìÖ Completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("\nüìÑ Generated Files:")
    print("  - reports/performance_summary.csv")
    print("  - reports/final_performance_report.md")
    print("  - reports/performance_charts.png (if matplotlib available)")
